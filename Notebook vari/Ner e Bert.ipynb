{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNzFMRRbOK6pDMPnpIsHWj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZL3tMJ2jopMP"},"outputs":[],"source":["#Ner con Bert\n","\n","from datasets import load_dataset\n","data = load_dataset(\"conllpp\")\n","\n","data[\"train\"][0]\n","\n","label_list = data[\"train\"].features[\"ner_tags\"].feature.names\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","example = data[\"train\"][0]\n","tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","\n","\n","def get_alignment(sequence):\n","        tokenized_inputs = tokenizer(sequence[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","        labels = []\n","        # iteriamo su tutti i ner tags della sequenza\n","        for i, label in enumerate(sequence[\"ner_tags\"]):\n","           word_ids = tokenized_inputs.word_ids(batch_index=i)  # Mappa i tokens alla parola corretta\n","           previous_word_idx = None\n","           label_ids = []\n","            for word_idx in word_ids:\n","               if word_idx is None:\n","                    label_ids.append(-100)\n","                elif word_idx != previous_word_idx:  # si assegna la label solo al primo dei token spezzati\n","                    label_ids.append(label[word_idx])\n","                else:\n","                  label_ids.append(-100)\n","               previous_word_idx = word_idx\n","            labels.append(label_ids)\n","\n","        tokenized_inputs[\"labels\"] = labels\n","        return tokenized_inputs\n","\n","tokenized_data = data.map(get_alignment, batched=True)\n","\n","\n","DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 14041\n","    }),\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 3250\n","    }),\n","    test: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 3453\n","    })\n","})\n","\n","tokenized_data['train'][0]\n","\n","\n","small_tr = tokenized_data['train'].select(range(5000))\n","small_ts = tokenized_data['test'].select(range(2500))\n","\n","\n","from transformers import DataCollatorForTokenClassification\n","data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors='tf')\n","\n","\n","\n","\n","DataCollatorForTokenClassification(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-uncased',\n","    vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right',\n","    truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]',\n","    'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'},\n","    clean_up_tokenization_spaces=True), padding=True, max_length=None,\n","    pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors=\"tf\")\n","\n","\n","import evaluate\n","seqeval = evaluate.load(\"seqeval\")\n","\n","import numpy as np\n","\n","labels = [label_list[i] for i in example[\"ner_tags\"]]\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","id2label = {\n","    0: 'O',\n","    1: 'B-PER',\n","    2: 'I-PER',\n","    3: 'B-ORG',\n","    4: 'I-ORG',\n","    5: 'B-LOC',\n","    6: 'I-LOC',\n","    7: 'B-MISC',\n","    8: 'I-MISC'\n","}\n","\n","label2id = {\n","    'O': 0,\n","    'B-PER': 1,\n","    'I-PER': 2,\n","    'B-ORG': 3,\n","    'I-ORG': 4,\n","    'B-LOC': 5,\n","    'I-LOC': 6,\n","    'B-MISC': 7,\n","    'I-MISC': 8\n","}\n","\n","from transformers import create_optimizer\n","\n","batch_size = 16\n","num_train_epochs = 3\n","num_train_steps = (len(small_tr) // batch_size) * num_train_epochs\n","optimizer, lr_schedule = create_optimizer(\n","    init_lr=2e-5,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n","    num_warmup_steps=0,\n",")\n","\n","from transformers import TFAutoModelForTokenClassification\n","\n","model = TFAutoModelForTokenClassification.from_pretrained(\n","    \"distilbert-base-uncased\", num_labels=9, id2label=id2label, label2id=label2id\n",")\n","\n"," tf_train_set = model.prepare_tf_dataset(\n","    small_tr,\n","    shuffle=True,\n","    batch_size=32,\n","    collate_fn=data_collator,\n",")\n","\n","tf_validation_set = model.prepare_tf_dataset(\n","    small_ts,\n","    shuffle=False,\n","    batch_size=32,\n","    collate_fn=data_collator,\n",")\n","\n"," from transformers.keras_callbacks import KerasMetricCallback\n","\n","metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n","\n","callbacks = [metric_callback]\n","\n","model.compile(optimizer=optimizer)\n","\n","model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=1, callbacks=callbacks)\n","\n","model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=1, callbacks=callbacks)\n","\n","\n","\n","from transformers import NerPipeline\n","text = \"Bill Gates and Microsoft will be the sponsors for one-day cricket international between Pakistan and New Zealand\"\n","print(text)\n","ner = NerPipeline(tokenizer=tokenizer, model=model, aggregation_strategy='simple')\n","ner(text)\n","\n","\n","\n"]}]}