{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+331os3hshqfuvfymlWhL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"af0sDnG5pPJW"},"outputs":[],"source":["#Preprocessing del Dataset di Flickr\n","\n","with open('image_features.pkl', 'rb') as f:\n","        image_features = pickle.load(f)\n","\n"," def text_preprocessing(data):\n","        data['caption'] = data['caption'].apply(lambda x: x.lower())\n","        data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) > 1]))\n","        data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n","        return data\n","\n","df_captions = text_preprocessing(df_captions)\n","captions = df_captions['caption'].tolist()\n","\n","import numpy as np\n","images, filename_index = [], []\n","for i, filename in enumerate(df_captions.image):\n","        if filename in image_features.keys():\n","            images.append(image_features[filename])\n","            filename_index.append(i)\n","\n","filenames = df_captions[\"image\"].iloc[filename_index].values\n","captions = df_captions[\"caption\"].iloc[filename_index].values\n","images = np.array(images)\n","\n","\n","from keras_preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words = 5000)\n","tokenizer.fit_on_texts(captions)\n","\n","sequences = tokenizer.texts_to_sequences(captions)\n","maxlen = len(max(sequences, key=len))\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","\n","\n","val_size = int(len(sequences) * 0.2)\n","test_size = int(len(sequences) * 0.2)\n","\n","def split_test_val_train(sequences, val_size, test_size):\n","    return (sequences[:test_size], sequences[test_size:test_size+val_size], sequences[test_size+val_size:])\n","\n","txt_test, txt_val, txt_train = split_test_val_train(sequences, val_size, test_size)\n","img_test, img_val, img_train = split_test_val_train(images, val_size, test_size)\n","filename_test, filename_val, filename_train = split_test_val_train(filenames, val_size, test_size)\n","\n","\n","prova = sequences[0]\n","\n","\n","\n","for i in range(1, len(prova)):\n","    seq_in = prova[:i]\n","    seq_out = prova[i]\n","    print(seq_in, seq_out)\n","\n"," def prepare_data(sequences, images):\n","        print(\"# Captions = {}\".format(len(sequences)))\n","\n","        X_seq, X_img, y_seq = [], [], []\n","        for seq, img in zip(sequences, images):\n","           for i in range(1, len(seq)):\n","\n","                seq_in, seq_out = seq[:i], seq[i]\n","                seq_in = pad_sequences([seq_in], maxlen=maxlen).flatten()\n","                seq_out = to_categorical(seq_out, num_classes=vocab_size)\n","                X_seq.append(seq_in)\n","                X_img.append(img)\n","                y_seq.append(seq_out)\n","\n","           X_seq = np.array(X_seq)\n","           X_img = np.array(X_img)\n","           y_seq = np.array(y_seq)\n","\n","        print(\"\\nShapes:\\n{} {} {}\".format(X_seq.shape, X_img.shape, y_seq.shape))\n","        return (X_seq, X_img, y_seq)"]}]}